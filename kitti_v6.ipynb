{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YX7gGIb1nZb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'/media/joseph/Development/SFITC/kitti/data_scene_flow/training/'\n",
    "RAW_DATA_PATH = r'2011_09_26/2011_09_26_drive_0057_sync'\n",
    "\n",
    "left_image_paths = sorted(glob(os.path.join(RAW_DATA_PATH, 'image_02/data/0000000125.png')))\n",
    "right_image_paths = sorted(glob(os.path.join(RAW_DATA_PATH, 'image_03/data/0000000125.png')))\n",
    "\n",
    "# get LiDAR data\n",
    "gt_paths = sorted(glob(os.path.join(DATA_PATH, 'disp_noc_0/000112_10.png')))\n",
    "\n",
    "lidar_paths = sorted(glob(os.path.join(RAW_DATA_PATH, 'velodyne_points/data/0000000125.bin')))\n",
    "\n",
    "print(f\"Number of left images: {len(left_image_paths)}\")\n",
    "print(f\"Number of right images: {len(right_image_paths)}\")\n",
    "print(f\"Number of LiDAR point clouds: {len(lidar_paths)}\")\n",
    "print(f\"Number of GT point clouds: {len(gt_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_paths = sorted(glob('/media/joseph/Development/SFITC/kitti/data_scene_flow_calib/training/calib_cam_to_cam/*.txt'))\n",
    "\n",
    "with open(calib_paths[0],'r') as f:\n",
    "    calib = f.readlines()\n",
    "\n",
    "# get projection matrices\n",
    "P_left = np.array([float(x) for x in calib[25].strip().split(' ')[1:]]).reshape((3,4))\n",
    "P_right = np.array([float(x) for x in calib[33].strip().split(' ')[1:]]).reshape((3,4))\n",
    "\n",
    "# get rectified rotation matrices\n",
    "R_left_rect = np.array([float(x) for x in calib[24].strip().split(' ')[1:]]).reshape((3, 3,))\n",
    "R_right_rect = np.array([float(x) for x in calib[32].strip().split(' ')[1:]]).reshape((3, 3,))\n",
    "\n",
    "R_left_rect = np.insert(R_left_rect, 3, values=[0,0,0], axis=0)\n",
    "R_left_rect = np.insert(R_left_rect, 3, values=[0,0,0,1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_projection_matrix(P):    \n",
    "    K, R, T, _, _, _, _ = cv2.decomposeProjectionMatrix(P)\n",
    "    T = T/T[3]\n",
    "\n",
    "    return K, R, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_left, R_left, T_left = decompose_projection_matrix(P_left)\n",
    "K_right, R_right, T_right = decompose_projection_matrix(P_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T_left)\n",
    "print(R_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_paths = sorted(glob('/media/joseph/Development/SFITC/kitti/data_scene_flow_calib/training/calib_velo_to_cam/*.txt'))\n",
    "\n",
    "with open(calib_paths[0], 'r') as f:\n",
    "    calib = f.readlines()\n",
    "\n",
    "R_cam_velo = np.array([float(x) for x in calib[1].strip().split(' ')[1:]]).reshape((3, 3))\n",
    "t_cam_velo = np.array([float(x) for x in calib[2].strip().split(' ')[1:]])[:, None]\n",
    "\n",
    "T_cam_velo = np.vstack((np.hstack((R_cam_velo, t_cam_velo)),\n",
    "                        np.array([0, 0, 0, 1])))\n",
    "\n",
    "T_cam_velo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "left_image = cv2.cvtColor(cv2.imread(left_image_paths[index]), cv2.COLOR_BGR2RGB)\n",
    "right_image = cv2.cvtColor(cv2.imread(right_image_paths[index]), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of an image is: {left_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(25, 15))\n",
    "ax1.imshow(left_image)\n",
    "ax1.set_title('Left Image', size=22)\n",
    "ax2.imshow(right_image)\n",
    "ax2.set_title('Right Image', size=22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sgbm_disparity(left_image, right_image, num_disparities=5*16, \n",
    "                           block_size=11, window_size=5, display=False):\n",
    "    \"\"\" Computes the disparity of an image pair using the SGBM algoithm.\n",
    "        Inputs: \n",
    "            image_left/_right - (MxN) grayscale input images\n",
    "            see opencv documentation for \"StereoBM_create\"\n",
    "        Outputs:\n",
    "            disparity (MxN) computed disparity map for the input images\n",
    "        \n",
    "        NOTE: image_left must be the left image (same for the right) or \n",
    "              unexpected results will occur due to \n",
    "    \"\"\"\n",
    "    # P1 and P2 control disparity smoothness (recommended values below)\n",
    "    P1 = 8 * 3 * window_size**2\n",
    "    P2 = 32 * 3 * window_size**2\n",
    "    sgbm_obj = cv2.StereoSGBM_create(0, num_disparities, block_size, \n",
    "        P1, P2, mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY)\n",
    "        \n",
    "    # compute disparity\n",
    "    disparity = sgbm_obj.compute(left_image, right_image).astype(np.float32)/16.0\n",
    "\n",
    "    # display is desired\n",
    "    if display:\n",
    "      plt.figure(figsize = (40,20))\n",
    "      plt.imshow(disparity, cmap='cividis')\n",
    "      plt.title('Disparity Map', size=25)\n",
    "      plt.show();\n",
    "\n",
    "    return disparity\n",
    "\n",
    "def calc_depth_map(disp_left, K_left, T_left, T_right):\n",
    "    ''' Computes Depth map from Intrinsic Camera Matrix and Translations vectors.\n",
    "        For KITTI, the depth is in meters.\n",
    "        '''\n",
    "    # Get the focal length from the K matrix\n",
    "    f = K_left[0, 0]\n",
    "    \n",
    "    # Get the distance between the cameras from the t matrices (baseline)\n",
    "    b = np.abs(T_left[0] - T_right[0])[0]\n",
    "    \n",
    "    # Replace all instances of 0 and -1 disparity with a small minimum value (to avoid div by 0 or negatives)\n",
    "    disp_left[disp_left <= 0] = 1e-5\n",
    "    \n",
    "    # Calculate the depths \n",
    "    depth_map = f*b / disp_left \n",
    "\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image_gray = cv2.cvtColor(left_image, cv2.COLOR_RGB2GRAY)\n",
    "right_image_gray = cv2.cvtColor(right_image, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "disparity = interactive(compute_sgbm_disparity, \n",
    "                        left_image=fixed(left_image_gray), \n",
    "                        right_image=fixed(right_image_gray), \n",
    "                        num_disparities=(0,512,16), \n",
    "                        block_size=(1,19,2), \n",
    "                        window_size=(1,13,2),\n",
    "                        display=fixed(True))\n",
    "display(disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_disparities = disparity.kwargs['num_disparities']\n",
    "block_size = disparity.kwargs['block_size']\n",
    "window_size = disparity.kwargs['window_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity = compute_sgbm_disparity(left_image_gray, \n",
    "                                   right_image_gray, \n",
    "                                   num_disparities, \n",
    "                                   block_size, \n",
    "                                   window_size, \n",
    "                                   display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = gt_paths[index]\n",
    "\n",
    "gt_disp = cv2.imread(gt, cv2.IMREAD_GRAYSCALE)\n",
    "# print(gt, type(gt_depth_map))\n",
    "\n",
    "gt_disp = gt_disp.astype(np.float32)\n",
    "# gt_disp[gt_disp == 0] = 1e-05\n",
    "\n",
    "gt_depth_map = calc_depth_map(gt_disp, K_left, T_left, T_right)\n",
    "gt_depth_map[gt_depth_map > 38000000.0] = np.NAN\n",
    "# gt_depth_map[gt_depth_map == 0] = np.NAN\n",
    "# gt_depth_map[gt_depth_map == np.Infinity] = np.NAN\n",
    "\n",
    "plt.imshow(gt_depth_map);\n",
    "gt_depth_map[50,600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velo2cam(lidar_bin):\n",
    "    ''' Converts the LiDAR point cloud to camera (u, v, z) image coordinates, \n",
    "        where z is in meters\n",
    "        '''\n",
    "    # read in LiDAR data\n",
    "    scan_data = np.fromfile(lidar_bin, dtype=np.float32).reshape((-1,4))\n",
    "\n",
    "    # convert to homogeneous coordinate system\n",
    "    velo_points = scan_data[:, 0:3] # (x, y, z) --> (front, left, up)\n",
    "    velo_points = np.insert(velo_points, 3, 1, axis=1).T # homogeneous LiDAR points\n",
    "\n",
    "    # delete negative liDAR points\n",
    "    velo_points = np.delete(velo_points, np.where(velo_points[3, :] < 0), axis=1) \n",
    "\n",
    "    # possibly use RANSAC to remove the ground plane for better viewing?\n",
    "\n",
    "    # convert to camera coordinates\n",
    "    velo_camera = P_left @ R_left_rect @ T_cam_velo @ velo_points\n",
    "\n",
    "    # delete negative camera points ??\n",
    "    velo_camera  = np.delete(velo_camera , np.where(velo_camera [2,:] < 0)[0], axis=1) \n",
    "\n",
    "    # get camera coordinates u,v,z\n",
    "    velo_camera[:2] /= velo_camera[2, :]\n",
    "\n",
    "    return velo_camera\n",
    "\n",
    "def project_velo2cam(lidar_bin, image):\n",
    "    ''' Projects LiDAR point cloud onto the image coordinate frame '''\n",
    "\n",
    "    # get camera (u, v, z) coordinates\n",
    "    velo_camera = get_velo2cam(lidar_bin)\n",
    "\n",
    "    (u, v, z) = velo_camera\n",
    "\n",
    "    # remove outliers (points outside of the image frame)\n",
    "    img_h, img_w, _ = image.shape\n",
    "    u_out = np.logical_or(u < 0, u > img_w)\n",
    "    v_out = np.logical_or(v < 0, v > img_h)\n",
    "    outlier = np.logical_or(u_out, v_out)\n",
    "    velo_camera = np.delete(velo_camera, np.where(outlier), axis=1)\n",
    "    \n",
    "    return velo_camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_bin = lidar_paths[index]\n",
    "(u, v, z) = project_velo2cam(lidar_bin, left_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uvz_to_depth_map(uvz, image_shape):\n",
    "    depth_map = np.zeros(image_shape, dtype=np.float32)\n",
    "    \n",
    "    u, v, z = uvz\n",
    "    u, v = u.astype(np.int32), v.astype(np.int32)\n",
    "    \n",
    "    valid_indices = (u >= 0) & (u < image_shape[1]) & (v >= 0) & (v < image_shape[0])\n",
    "    u, v, z = u[valid_indices], v[valid_indices], z[valid_indices]\n",
    "    \n",
    "    depth_map[v, u] = z\n",
    "    \n",
    "    return depth_map\n",
    "    \n",
    "lidar_depth_map = uvz_to_depth_map((u, v, z), stereo_depth_map.shape)\n",
    "\n",
    "# TODO: Why not working?\n",
    "lidar_depth_map[lidar_depth_map == 0] = np.NAN\n",
    "\n",
    "plt.imshow(lidar_depth_map, cmap='rainbow_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_depth_map = calc_depth_map(disparity, K_left, T_left, T_right)\n",
    "stereo_depth_map[stereo_depth_map > 38000000.0] = np.NAN\n",
    "\n",
    "plt.imshow(stereo_depth_map, cmap='rainbow_r'); # or 'cividis_r' or 'cividis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_depth_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make this dynamic\n",
    "\n",
    "plt.imshow(lidar_depth_map[150:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(stereo_depth_map[:,80:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_map_to_point_cloud(depth_map, flip=True):\n",
    "    # Convert the depth map to Open3D depth image\n",
    "    depth_image_o3d = o3d.geometry.Image(depth_map.astype(np.float32))\n",
    "\n",
    "    # Create an Intrinsics object using camera parameters\n",
    "    height, width = depth_map.shape\n",
    "    fx, fy = K_left[0, 0], K_left[1, 1]\n",
    "    cx, cy = K_left[0, 2], K_left[1, 2]\n",
    "    intrinsic = o3d.camera.PinholeCameraIntrinsic(\n",
    "        width, height, fx, fy, cx, cy)\n",
    "\n",
    "    # Create a point cloud from the depth image\n",
    "    point_cloud = o3d.geometry.PointCloud.create_from_depth_image(\n",
    "        depth_image_o3d, \n",
    "        intrinsic,\n",
    "        # depth_scale=1,\n",
    "        # depth_trunc=0.00070\n",
    "    )\n",
    "\n",
    "    if flip:\n",
    "        point_cloud.transform([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, -1, 0, 0],\n",
    "            [0, 0, -1, 0],\n",
    "            [0, 0, 0, 1]\n",
    "        ])\n",
    "\n",
    "    return point_cloud\n",
    "\n",
    "\n",
    "def rgbd_to_point_cloud(color, depth_map, flip=True):\n",
    "    # Convert the depth map to Open3D depth image\n",
    "    depth_image_o3d = o3d.geometry.Image(depth_map.astype(np.float32))\n",
    "\n",
    "    # Convert the depth map to Open3D depth image\n",
    "    color_image_o3d = o3d.geometry.Image(color)\n",
    "\n",
    "    # Create RGBD image\n",
    "    rgbd_image_o3d = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        color_image_o3d, \n",
    "        depth_image_o3d,\n",
    "        depth_scale=1,\n",
    "        depth_trunc=70\n",
    "        )\n",
    "\n",
    "    # Create an Intrinsics object using camera parameters\n",
    "    height, width = depth_map.shape\n",
    "    fx, fy = K_left[0, 0], K_left[1, 1]\n",
    "    cx, cy = K_left[0, 2], K_left[1, 2]\n",
    "    intrinsic = o3d.camera.PinholeCameraIntrinsic(\n",
    "        width, height, fx, fy, cx, cy)\n",
    "\n",
    "    # Create a point cloud from the depth image\n",
    "    point_cloud = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "        rgbd_image_o3d, intrinsic)\n",
    "\n",
    "    if flip:\n",
    "        point_cloud.transform([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, -1, 0, 0],\n",
    "            [0, 0, -1, 0],\n",
    "            [0, 0, 0, 1]\n",
    "        ])\n",
    "\n",
    "    return point_cloud\n",
    "\n",
    "# # Visualize the point cloud\n",
    "# o3d.visualization.draw_geometries([\n",
    "#     # depth_map_to_point_cloud(stereo_depth_map),\n",
    "#     depth_map_to_point_cloud(lidar_depth_map),\n",
    "#     rgbd_to_point_cloud(left_image, stereo_depth_map),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "# vlp_depth = cp.array(lidar_depth_map)\n",
    "# zed_depth = cp.array(stereo_depth_map)\n",
    "# rgb = cp.array(left_image)\n",
    "gt_depth = cp.array(gt_depth_map[150:,80:])\n",
    "vlp_depth = cp.array(lidar_depth_map[150:,80:])\n",
    "zed_depth = cp.array(stereo_depth_map[150:,80:])\n",
    "rgb = cp.array(left_image[150:,80:])\n",
    "\n",
    "def fillB_withA(A, B):\n",
    "    # Combine A and B, prioritizing non-NaN values in A\n",
    "    combined = cp.where(cp.isnan(A), B, A)\n",
    "\n",
    "    # Calculate the column-wise means of the combined matrix, ignoring NaNs\n",
    "    col_means = cp.nanmean(combined, axis=0)\n",
    "\n",
    "    # Replace NaN values in the combined matrix with the column-wise means\n",
    "    combined = cp.where(cp.isnan(combined), col_means, combined)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(cp.count_nonzero(cp.isnan(vlp_depth)))\n",
    "print(cp.count_nonzero(cp.isnan(zed_depth)))\n",
    "\n",
    "mask = cp.isnan(zed_depth)\n",
    "\n",
    "zed_depth = cp.array(cv2.inpaint(zed_depth.get(), mask.get().astype(np.uint8), 3, cv2.INPAINT_TELEA))\n",
    "\n",
    "# zed_depth[cp.isnan(zed_depth)] = vlp_depth.mean()\n",
    "# zed_depth[cp.isnan(zed_depth)] = vlp_depth.max()\n",
    "# zed_depth[cp.isnan(zed_depth)] = vlp_depth.min()\n",
    "# zed_depth[zed_depth > 2000] = vlp_mean\n",
    "\n",
    "zed_depth = fillB_withA(vlp_depth, zed_depth)\n",
    "\n",
    "print(cp.count_nonzero(cp.isnan(zed_depth)))\n",
    "\n",
    "plt.imshow(np.log(zed_depth.get()), cmap='rainbow_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpf(img, ncutoff):\n",
    "    # Apply 2D FFT to the image\n",
    "    f = cp.fft.fft2(img)\n",
    "\n",
    "    # Shift the zero frequency component to the center of the spectrum\n",
    "    fshift = cp.fft.fftshift(f)\n",
    "\n",
    "    # Create a circular mask of the same size as the spectrum\n",
    "    rows, cols = img.shape\n",
    "    crow, ccol = rows // 2, cols // 2\n",
    "    mask = np.zeros((rows, cols), np.uint8)\n",
    "    cutoff = int(min(crow, ccol)*ncutoff)\n",
    "    cv2.circle(mask, (ccol, crow), cutoff, 1, -1)\n",
    "    # cv2.ellipse(mask, (ccol, crow), (1, 2) * cutoff, 0, 0, 360,  1, -1)\n",
    "\n",
    "    mask = cp.asarray(mask)\n",
    "\n",
    "    # Apply the mask to the shifted spectrum\n",
    "    fshift_filtered = fshift * mask\n",
    "\n",
    "    # Shift the zero frequency component back to the corner of the spectrum\n",
    "    f_filtered = cp.fft.ifftshift(fshift_filtered)\n",
    "\n",
    "    # Apply the inverse 2D FFT to the filtered spectrum\n",
    "    img_filtered = cp.fft.ifft2(f_filtered)\n",
    "    img_filtered = cp.real(img_filtered)\n",
    "\n",
    "    return img_filtered\n",
    "\n",
    "\n",
    "def pg(zed_depth, vlp_depth, ncutoff, threshold=100):\n",
    "    mask = vlp_depth > 0\n",
    "    filtered = zed_depth\n",
    "\n",
    "    while threshold > 0:\n",
    "        filtered[mask] = vlp_depth[mask]\n",
    "        filtered = lpf(filtered, ncutoff)\n",
    "\n",
    "        threshold -= 1\n",
    "        # ncutoff = ncutoff / 10\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "pg_depth = pg(\n",
    "    cp.array(zed_depth.copy()),\n",
    "    cp.array(vlp_depth.copy()),\n",
    "    # cp.log(cp.array(zed_depth.copy())),\n",
    "    # cp.log(cp.array(vlp_depth.copy())),\n",
    "    ncutoff=10,\n",
    "    threshold=1000\n",
    ")\n",
    "\n",
    "# pg_depth = cp.exp(pg_depth)\n",
    "pg_depth[mask] = cp.NAN\n",
    "masked_zed_depth = zed_depth\n",
    "masked_zed_depth[mask] = cp.NAN\n",
    "\n",
    "# plt.imshow(pg_depth.get(), cmap='rainbow_r')\n",
    "# plt.imshow(np.log(pg_depth.get()), cmap='rainbow_r')\n",
    "# plt.imshow(np.exp(pg_depth.get()), cmap='rainbow_r')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(np.log(masked_zed_depth.get()), cmap='rainbow_r')\n",
    "plt.title('Zed')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(np.log(pg_depth.get()), cmap='rainbow_r')\n",
    "plt.title('PG')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(np.log(pg_depth.get() - zed_depth.get()), cmap='rainbow_r')\n",
    "plt.title('Diff')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the point cloud\n",
    "# o3d.visualization.draw_geometries([\n",
    "#     # depth_map_to_point_cloud(zed_depth.get()),\n",
    "#     rgbd_to_point_cloud(rgb.get(), masked_zed_depth.get()),\n",
    "#     # depth_map_to_point_cloud(vlp_depth.get()),\n",
    "#     # depth_map_to_point_cloud(gt_depth.get()),\n",
    "#     # rgbd_to_point_cloud(rgb.get(), pg_depth.get()),\n",
    "#     # depth_map_to_point_cloud(pg_depth.get()),\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Error (MAE) between predicted and ground truth images,\n",
    "    ignoring NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    predicted (numpy.ndarray): Predicted depth image.\n",
    "    ground_truth (numpy.ndarray): Ground truth depth image.\n",
    "\n",
    "    Returns:\n",
    "    float: The MAE value.\n",
    "    \"\"\"\n",
    "    # Ensure the images have the same shape\n",
    "    if predicted.shape != ground_truth.shape:\n",
    "        raise ValueError(\"Predicted and ground truth images must have the same shape.\")\n",
    "\n",
    "    # Create a mask for valid (non-NaN) pixels\n",
    "    valid_mask = ~np.isnan(predicted) & ~np.isnan(ground_truth)\n",
    "\n",
    "    # Calculate the absolute differences only for valid pixels\n",
    "    diffs = np.abs(predicted[valid_mask] - ground_truth[valid_mask])\n",
    "\n",
    "    # Calculate the mean of these differences\n",
    "    mae = np.mean(diffs)\n",
    "    return mae\n",
    "\n",
    "print(calculate_mae(gt_depth.get(), gt_depth.get()))\n",
    "print(calculate_mae(pg_depth.get(), gt_depth.get()))\n",
    "print(calculate_mae(masked_zed_depth.get(), gt_depth.get()))\n",
    "print(calculate_mae(vlp_depth.get(), gt_depth.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_custom_mae(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate a custom Mean Absolute Error (MAE) between predicted and ground truth images.\n",
    "    NaN values in the ground truth are ignored. NaN values in the predicted image, where\n",
    "    the ground truth is not NaN, are penalized.\n",
    "\n",
    "    Parameters:\n",
    "    predicted (numpy.ndarray): Predicted depth image.\n",
    "    ground_truth (numpy.ndarray): Ground truth depth image.\n",
    "\n",
    "    Returns:\n",
    "    float: The custom MAE value.\n",
    "    \"\"\"\n",
    "    # Ensure the images have the same shape\n",
    "    if predicted.shape != ground_truth.shape:\n",
    "        raise ValueError(\"Predicted and ground truth images must have the same shape.\")\n",
    "\n",
    "    # Mask for valid pixels in ground truth\n",
    "    valid_gt_mask = ~np.isnan(ground_truth)\n",
    "\n",
    "    # Mask for pixels where predicted is NaN but ground truth is not\n",
    "    bad_pred_mask = np.isnan(predicted) & valid_gt_mask\n",
    "\n",
    "    # Assign a large error value for bad predictions\n",
    "    bad_pred_penalty = 1000  # or some other large number representing a high error\n",
    "\n",
    "    # Calculate absolute differences where both are valid and add penalty for bad predictions\n",
    "    diffs = np.abs(np.where(bad_pred_mask, bad_pred_penalty, predicted) - ground_truth)\n",
    "\n",
    "    # Calculate the mean over the ground truth valid mask\n",
    "    mae = np.mean(diffs[valid_gt_mask])\n",
    "    return mae\n",
    "\n",
    "print(calculate_custom_mae(gt_depth.get(), gt_depth.get()))\n",
    "print(calculate_custom_mae(pg_depth.get(), gt_depth.get()))\n",
    "print(calculate_custom_mae(masked_zed_depth.get(), gt_depth.get()))\n",
    "print(calculate_custom_mae(vlp_depth.get(), gt_depth.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nan_to_num(pg_depth.get() - masked_zed_depth.get(), 0).max())\n",
    "print(np.nan_to_num(gt_depth.get() - masked_zed_depth.get(), 0).max())\n",
    "print(np.nan_to_num(vlp_depth.get() - masked_zed_depth.get(), 0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Square Error (RMSE) between predicted and ground truth images,\n",
    "    taking into account NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    predicted (numpy.ndarray): Predicted depth image.\n",
    "    ground_truth (numpy.ndarray): Ground truth depth image.\n",
    "\n",
    "    Returns:\n",
    "    float: The RMSE value.\n",
    "    \"\"\"\n",
    "    # Ensure the images have the same shape\n",
    "    if predicted.shape != ground_truth.shape:\n",
    "        raise ValueError(\"Predicted and ground truth images must have the same shape.\")\n",
    "\n",
    "    # Mask for valid pixels in ground truth\n",
    "    valid_gt_mask = ~np.isnan(ground_truth)\n",
    "\n",
    "    # Mask for pixels where predicted is NaN but ground truth is not\n",
    "    bad_pred_mask = np.isnan(predicted) & valid_gt_mask\n",
    "\n",
    "    # Assign a large error value for bad predictions\n",
    "    bad_pred_penalty = 1000  # or some other large number representing a high error\n",
    "\n",
    "    # Calculate squared differences where both are valid and add penalty for bad predictions\n",
    "    squared_diffs = (np.where(bad_pred_mask, bad_pred_penalty, predicted) - ground_truth) ** 2\n",
    "\n",
    "    # Calculate the mean of the squared differences over the ground truth valid mask and then take the square root\n",
    "    rmse = np.sqrt(np.mean(squared_diffs[valid_gt_mask]))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "print(calculate_rmse(gt_depth.get(), gt_depth.get()))\n",
    "print(calculate_rmse(pg_depth.get(), gt_depth.get()))\n",
    "print(calculate_rmse(masked_zed_depth.get(), gt_depth.get()))\n",
    "print(calculate_rmse(vlp_depth.get(), gt_depth.get()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('Python-ddiOOh4g')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00f03ad223d9bbefba2d85a96e4c14bf4b7cfec3ac1501740897c115facd9986"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
